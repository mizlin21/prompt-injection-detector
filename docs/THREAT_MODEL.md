# Threat Model — Prompt Injection Detector (PID)

## 1. System Overview

PID is a guardrail component designed to detect prompt injection attempts
before untrusted content is executed by a Large Language Model (LLM).

It operates at the boundary between:
- user input
- retrieved documents (RAG)
- tool / function outputs
- model execution

PID is **preventive**, not reactive — it reasons about intent before model inference.

PID is intended to be deployed as a pre-inference guardrail in LLM pipelines.

---

## 2. Assets to Protect

The primary assets PID helps protect are:

- System instructions and developer prompts
- Tool invocation integrity (functions, APIs, agents)
- Sensitive secrets (API keys, tokens, credentials)
- Application control flow
- LLM behavior predictability
- User trust and application safety

---

## 3. Trust Boundaries

PID explicitly models different trust levels:

| Source          | Trust Level | Rationale |
|-----------------|-------------|-----------|
| user input      | low         | Fully attacker-controlled |
| retrieved_doc   | medium-low  | Indirect attacker influence via data poisoning |
| tool_output     | medium      | Generated by system but influenced by untrusted input |
| system prompt   | high        | Must never be influenced by external content |

Crossing these boundaries without inspection is considered **unsafe by design**.

---

## 4. Attacker Model

PID assumes an attacker who can:

- submit crafted user prompts
- poison RAG data sources
- influence retrieved context
- attempt to escalate roles or authority
- force tool or function execution
- exfiltrate hidden system instructions
- chain multiple weak signals to bypass safeguards

The attacker is assumed to have:
- no direct system access
- no code execution privileges
- but full control of input content

The attacker cannot directly modify PID logic, but may attempt to influence
scoring outcomes through crafted content and context manipulation.

---

## 5. Threats Considered

PID explicitly models the following threats:

| Threat | Description |
|------|------------|
| Instruction override | Attempts to bypass system rules |
| Secret exfiltration | Attempts to extract hidden prompts or credentials |
| Authority coercion | Role impersonation (admin, system, developer) |
| Tool hijacking | Forcing function or API execution |
| RAG poisoning | Malicious content embedded in retrieved documents |
| Obfuscation / evasion | Attempts to hide malicious intent |

Each threat is mapped to an **attack intent taxonomy** and weighted during scoring.

---

## 6. Detection Strategy

PID uses a layered approach:

1. Normalize input (reduce obfuscation variance)
2. Apply explicit rules mapped to attack intents
3. Classify each finding by intent
4. Apply intent-aware weighting
5. Apply source-aware weighting
6. Aggregate to a risk score
7. Decide allow / review / block based on profile

This allows PID to reason about **what the attacker is trying to do**, not just what they typed.

All detection decisions are deterministic and reproducible for audit purposes.

---

## 7. Out of Scope (Non-Goals)

PID does NOT attempt to:

- Fully prevent all prompt injection
- Replace model-side safety training
- Detect semantic manipulation without signals
- Handle adversarial encoding (Unicode homoglyphs, steganography)
- Classify intent perfectly in isolation

These are explicitly deferred to future work or complementary controls.

---

## 8. Failure Modes & Limitations

Known limitations include:

- False negatives for novel phrasing
- False positives for legitimate instructions
- Partial coverage of multilingual attacks
- Rule-based patterns can be bypassed by creative attackers
- Intent classification depends on rule quality
- Compound attacks combining weak signals may still evade thresholds

These limitations are **documented by design**, not hidden.

---

## 9. Security Philosophy

PID is designed as a **transparent guardrail**, not a black box filter.

Security decisions must be:
- explainable
- auditable
- testable
- enforceable in CI
- reasoned from threat models

The goal is safe enablement of LLMs — not blanket restriction.

---

## 10. Framework Alignment (Lightweight)

This threat model aligns with established security frameworks while remaining
lightweight and practical for a single-component system.

### STRIDE Mapping
| STRIDE Category | PID Threat |
|-----------------|------------|
| Spoofing | Authority coercion (role impersonation) |
| Tampering | Instruction override, RAG poisoning |
| Repudiation | Out of scope (handled by logging layer) |
| Information Disclosure | Secret exfiltration |
| Denial of Service | Out of scope |
| Elevation of Privilege | Tool hijacking, instruction override |

### MITRE ATLAS / ATT&CK Alignment (Conceptual)
PID’s attack intents map conceptually to adversarial ML tactics:

- **Prompt Injection** → Initial Access / Execution
- **Instruction Override** → Privilege Escalation
- **Secret Exfiltration** → Credential Access
- **Tool Hijacking** → Lateral Movement
- **RAG Poisoning** → Persistence

This alignment is used for threat reasoning and stakeholder communication,
not as a rigid compliance taxonomy.


### Privacy Considerations (LINDDUN)
PID minimizes privacy risk by:
- avoiding prompt storage by default
- limiting logging to spans and metadata
- not retaining full content unless explicitly configured